---
title: "marketMover"
subtitle: "Prediction of Up/Down Movements in the S&P500 (Random Forest)"
author: "Robert Aranha"
date: "August 2021"
output: 
  html_document:
    toc: True
    toc_float: True
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Load Libraries

```{r load libraries}
library(tidyverse)
library(tidymodels)
library(ISLR)
library(knitr)
library(GGally)
library(doParallel)
library(glmnet)
library(reshape2)
library(kableExtra)
library(xgboost)
library(ranger)
library(kernlab)
library(rlist)
```

## Load Data

```{r load data}
# Documentation for data available here:
# https://www.rdocumentation.org/packages/ISLR/versions/1.2/topics/Smarket
data(Smarket)
market_dt <- Smarket %>%
  as_tibble() %>%
  select(-Today) %>%
  mutate(Direction = fct_relevel(Direction, 'Up'))

market_dt <- market_dt %>%
  mutate(cc_Lag1 = log(1 + Lag1/100),
         cc_avg = cummean(cc_Lag1),
         cc_over_avg = cc_Lag1 > cc_avg,
         cc_avg_diff = cc_Lag1 - cc_avg)
 
vals <- list()
 
for (row in 2:nrow(market_dt)) {
  cur_cc_over_avg <- market_dt[row, "cc_over_avg"]
  prev_cc_over_avg <- market_dt[row - 1, "cc_over_avg"]
  
  if (row == 2){
   if (prev_cc_over_avg == TRUE) {
     vals <- list.append(1)
   } else if (prev_cc_over_avg == FALSE) {
     vals <- list.append(-1)
   }
  }
  
  if(cur_cc_over_avg == TRUE && prev_cc_over_avg == TRUE){
   vals <- list.append(vals, 1 + vals[row - 1])
  } else if (cur_cc_over_avg == TRUE && prev_cc_over_avg == FALSE){
   vals <- list.append(vals, 1)   
  } else if (cur_cc_over_avg == FALSE && prev_cc_over_avg == TRUE){
   vals <- list.append(vals, -1)   
  } else if (cur_cc_over_avg == FALSE && prev_cc_over_avg == FALSE){
   vals <- list.append(vals, vals[row - 1] - 1)   
  }
}

market_dt$days_over_avg = vals
```

## Split Data

```{r split_data}
set.seed(123)
market_split <- initial_split(market_dt, prop=0.7, strata=Direction)
train_dt <- training(market_split)
test_dt <- testing(market_split)

set.seed(456)
folds_dt <- train_dt %>%
  vfold_cv(v=5, repeats=5, strata=Direction)
```

## Define Performance Metrics

```{r build_metric_set}
perf_meas <- metric_set(roc_auc, precision, recall, accuracy, f_meas, kap)
```

### Recipe

We now define the recipe for our random forest models. We will attempt to predict direction based on Volume, Year, cc_avg_diff, and days_over_avg. We will opt not to include time lags as a variable in our predictions for the rational stated in the EDA section.

We will apply the following transformations in our recipe:

+ *step_bagimpute* (creates bagged tree models to impute missing data): This step is applied as missing values will result in a loss of predictive performance in our models.
+ *step_zv* (removes of columns with single values): This step is applied as columns with single variables will add no predictive power to our models, particularly when working with decision trees.
+ *step_corr* (filters out predictors that are highly correlated):  This step is applied to reduce dependencies on background variables and reduce unneeded splits in our trees

Note that the only transformation applied to out outcome variables is ``step_bagimpute``. We will not consider applying other transformations to our outcome variable as we are working with categorical data. 

```{r rf_recipe}
rf_rec <- 
  recipe(Direction ~ Volume + Year + days_over_avg + cc_avg_diff, data=train_dt) %>%
  step_mutate(direction_na = ifelse(is.na(Direction), 1, 0), skip = TRUE)%>%
  step_bagimpute(Direction, Volume) %>%
  step_zv(all_predictors(), -all_outcomes()) %>%
  step_corr(all_predictors(), -all_outcomes())
```

### Define the Model

Our model will use the ranger engine and will be set to classification mode. We will indicate that we want to tune mtry and trees as hyperparameters. Note that trees refers to the number of trees in our ensemble method and mtry refers to the number of variables that are randomly sampled as candidates at each split in a tree.

```{r rf_model}
rf_mod <- 
  rand_forest(trees = tune(), mtry = tune()) %>%
  set_mode('classification') %>%
  set_engine('ranger')
```

## Hyperparameters

We then create a grid of hyperparameters related to the number of trees in our ensemble and the number of variables that should be randomly sampled as candidates at each split in a tree.

```{r rf_tune_hyperparameters}
rf_grid <- grid_latin_hypercube(
                trees(c(5, 100)),
                finalize(mtry(), train_dt),
                size = 20)
```

We then define the workflow, adding our random forest model and our recipe.

### Define Workflow
```{r rf_workflow}
rf_wf <- 
  workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_mod)
```

We then tune our model using our grid of hyperparameters.

### Tune Grid

```{r}
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
```

```{r rf_tune_grid}
tune_ctrl <- control_resamples(save_pred = TRUE)

rf_res <-
  rf_wf %>%
  tune_grid(
    folds_dt, 
    grid = rf_grid,
    metrics = perf_meas, 
    control = tune_ctrl
  )
```

```{r}
stopCluster(cl)
```

## Cross-Validation Results

At this point we have a series of model configurations that have been trained using our k-fold dataset. Below is a chart of our performance metrics show for each model configuration.

```{r rf_model_assessment}
rf_res %>%
  collect_metrics() %>%
  ggplot(aes(x = trees, y = mean)) +
  geom_line() +
  geom_errorbar(
    aes(ymin = mean - std_err,
        ymax = mean + std_err)
  ) +
  facet_wrap(~.metric, scales = 'free_y', ncol = 2) +
  theme_minimal() + 
  labs(title = 'Random Forest Performance', 
       subtitle = 'Tuning Trees and Number of Sample Features', 
       x = 'N Sampled Attributes', y = 'Performance Score')
```


## Top Configurations

### Selection of Best Model

On the basis of roc_auc the top performing model is ``mode11`` which contains 43 trees with mtry set to 6. The top performing models are shown below.

```{r rf_show_best}
rf_top <- rf_res %>%
  show_best(metric = 'roc_auc')

rf_top
```

```{r rf_select_best}
rf_best <- 
  rf_res %>%
  select_best(metric = 'roc_auc')
```

A confusion matrix summarizing our top models predictive performance is shown below.

```{r rf_confusion_matrix}
rf_confusion <-
  rf_res %>%
  conf_mat_resampled(parameters = rf_best)

rf_confusion
```

### Store Results

For future reference we will store the results from this section of our analysis in an external file.

```{r collect_rf_metrics}
rf_metrics <-
  rf_res %>%
  collect_metrics()
```

```{r rf_save_results}
save(rf_wf, rf_metrics, rf_confusion, rf_best, rf_top,
     file = '../data/forest.Rda')
```