---
title: "marketMover"
subtitle: "Prediction of Up/Down Movements in the S&P500 (SVMs)"
author: "Robert Aranha"
date: "August 2021"
output: 
  html_document:
    toc: True
    toc_float: True
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Load Libraries

```{r load libraries}
library(tidyverse)
library(tidymodels)
library(ISLR)
library(knitr)
library(GGally)
library(doParallel)
library(glmnet)
library(reshape2)
library(kableExtra)
library(xgboost)
library(ranger)
library(kernlab)
library(rlist)
```

## Load Data

```{r load data}
# Documentation for data available here:
# https://www.rdocumentation.org/packages/ISLR/versions/1.2/topics/Smarket
data(Smarket)
market_dt <- Smarket %>%
  as_tibble() %>%
  select(-Today) %>%
  mutate(Direction = fct_relevel(Direction, 'Up'))

market_dt <- market_dt %>%
  mutate(cc_Lag1 = log(1 + Lag1/100),
         cc_avg = cummean(cc_Lag1),
         cc_over_avg = cc_Lag1 > cc_avg,
         cc_avg_diff = cc_Lag1 - cc_avg)
 
vals <- list()
 
for (row in 2:nrow(market_dt)) {
  cur_cc_over_avg <- market_dt[row, "cc_over_avg"]
  prev_cc_over_avg <- market_dt[row - 1, "cc_over_avg"]
  
  if (row == 2){
   if (prev_cc_over_avg == TRUE) {
     vals <- list.append(1)
   } else if (prev_cc_over_avg == FALSE) {
     vals <- list.append(-1)
   }
  }
  
  if(cur_cc_over_avg == TRUE && prev_cc_over_avg == TRUE){
   vals <- list.append(vals, 1 + vals[row - 1])
  } else if (cur_cc_over_avg == TRUE && prev_cc_over_avg == FALSE){
   vals <- list.append(vals, 1)   
  } else if (cur_cc_over_avg == FALSE && prev_cc_over_avg == TRUE){
   vals <- list.append(vals, -1)   
  } else if (cur_cc_over_avg == FALSE && prev_cc_over_avg == FALSE){
   vals <- list.append(vals, vals[row - 1] - 1)   
  }
}

market_dt$days_over_avg = vals
```

## Split Data

```{r split_data}
set.seed(123)
market_split <- initial_split(market_dt, prop=0.7, strata=Direction)
train_dt <- training(market_split)
test_dt <- testing(market_split)

set.seed(456)
folds_dt <- train_dt %>%
  vfold_cv(v=5, repeats=5, strata=Direction)
```

## Define Performance Metrics

```{r build_metric_set}
perf_meas <- metric_set(roc_auc, precision, recall, accuracy, f_meas, kap)
```

### Recipe

We now define the recipe for our SVM model. We will attempt to predict direction based on Volume, Year, cc_avg_diff, and days_over_avg. We will opt not to include time lags as a variable in our predictions for the rational stated in the EDA section.

We will apply the following transformations in our recipe:

+ *step_bagimpute* (creates bagged tree models to impute missing data): This step is applied as missing values will result in a loss of predictive performance in our models.
+ *step_zv* (removes of columns with single values): This step is applied as columns with single variables will add no predictive power to our models.
+ *step_corr* (filters out predictors that are highly correlated):  This step is applied to reduce dependencies on background variables.
+ *step_normalize* (normalizes numeric data to have standard deviation of 1 and mean of 0):  This step is applied to scale predictors without distorting values or losing information.
+ *step_BoxCox* (transforms non-normal dependent variables into a normal shape): This step is applied to reduce skewness in predictors as the tails of the distribution can dominate the underlying calculations for SVMs.

Note that the only transformation applied to out outcome variables is ``step_bagimpute``. We will not consider applying other transformations to our outcome variable as we are working with categorical data. 

```{r svm_recipe}
svm_rec <- 
  recipe(Direction ~ Volume + Year + days_over_avg + cc_avg_diff, data=train_dt) %>%
  step_mutate(direction_na = ifelse(is.na(Direction), 1, 0), skip = TRUE)%>%
  step_bagimpute(Direction, Volume) %>%
  step_zv(all_predictors(), -all_outcomes()) %>%
  step_corr(all_predictors(), -all_outcomes()) %>%
  step_normalize(all_predictors(), -all_outcomes()) %>%
  step_BoxCox(all_predictors(), -all_outcomes())
```

### Define the Model

Our model will use the kernlab engine and will be set to classification mode. We will indicate that we want to tune rbf_sigma as a hyperparameter. Note that rbf_sigma refers to the standard deviation / size of the kernel.

```{r svm_model}
svm_mod <- 
  svm_rbf(rbf_sigma = tune()) %>%
  set_mode('classification') %>%
  set_engine('kernlab')
```

## Hyperparameters

We then create a grid of hyperparameters related to the rbf_sigma values we want to test our models performance across.

```{r svm_tune_hyperparameters}
svm_grid <- grid_regular(rbf_sigma(range = c(-9, 0), trans=log10_trans()), levels =10)
```

We then define the workflow, adding our svm model and our recipe.

### Define Workflow
```{r svm_workflow}
svm_wf <- 
  workflow() %>%
  add_recipe(svm_rec) %>%
  add_model(svm_mod)
```

We then tune our model using our grid of hyperparameters.

### Tune Grid

```{r}
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
```

```{r svm_tune_grid}
tune_ctrl <- control_resamples(save_pred = TRUE)

svm_res <-
  svm_wf %>%
  tune_grid(
    folds_dt, 
    grid = svm_grid,
    metrics = perf_meas, 
    control = tune_ctrl
  )
```

```{r}
stopCluster(cl)
```

## Cross-Validation Results

At this point we have a series of model configurations that have been trained using our k-fold dataset. Below is a chart of our performance metrics show for each model configuration.

```{r svm_model_assessment}
svm_res %>%
  collect_metrics() %>%
  ggplot(aes(x = rbf_sigma, y = mean)) +
  geom_line() +
  geom_errorbar(
    aes(ymin = mean - std_err,
        ymax = mean + std_err)
  ) +
  scale_x_log10() + 
  facet_wrap(~.metric, scales = 'free_y', ncol = 2) +
  theme_minimal() + 
  labs(title = 'Support Vector Machine Performance', 
       subtitle = 'Tuning RBF Sigma', 
       x = 'RBF Sigma', y = 'Performance Score')
```

## Top Configuration

### Selection of Best Model

On the basis of roc_auc the top performing model is ``model10`` where rbf_sigma is set to 1. The top performing models are shown below.

```{r svm_show_best}
svm_top <- svm_res %>%
  show_best(metric = 'roc_auc')

svm_top
```

```{r svm_select_best}
svm_best <- 
  svm_res %>%
  select_best(metric = 'roc_auc')
```

A confusion matrix summarizing our top models predictive performance is shown below.

```{r svm_confusion_matrix}
svm_confusion <-
  svm_res %>%
  conf_mat_resampled(parameters = svm_best)

svm_confusion
```

### Store Results

For future reference we will store the results from this section of our analysis in an external file.

```{r collect_svm_metrics}
svm_metrics <-
  svm_res %>%
  collect_metrics()
```

```{r svm_save_results}
save(svm_wf, svm_metrics, svm_confusion, svm_best, svm_top,
     file = '../data/svm.Rda')
```